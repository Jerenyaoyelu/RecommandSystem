{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:12:17.818547Z",
     "start_time": "2019-08-08T01:12:14.828989Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:12:17.833166Z",
     "start_time": "2019-08-08T01:12:17.822155Z"
    }
   },
   "outputs": [],
   "source": [
    "# embedding_size: the length of the hidden vector for both user and item\n",
    "emdedding_size_mlp = 20\n",
    "\n",
    "# batch_size: the number of cases put into our model for one term\n",
    "batch_size = 100\n",
    "\n",
    "# num_epoch: the total round we train our model\n",
    "num_epoch = 100\n",
    "\n",
    "# learing_rate: the variable chaging speed for our optimizer\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:12:18.354379Z",
     "start_time": "2019-08-08T01:12:17.840466Z"
    }
   },
   "outputs": [],
   "source": [
    "header = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "\n",
    "with open('ratings.csv') as file:\n",
    "    data = []\n",
    "    count = 0\n",
    "    csv_file = csv.reader(file)\n",
    "    for line in csv_file:\n",
    "        if count ==0:\n",
    "            count+= 1\n",
    "            continue\n",
    "        else:\n",
    "            data.append(list(map(float,line)))\n",
    "    data = np.array(data)\n",
    "\n",
    "n_users = int(max(data[:,0]))\n",
    "n_items = int(max(data[:,1]))\n",
    "# split our whole data into train_data and test_dataã€‚\n",
    "train_data, test_data = train_test_split(data, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:12:19.755461Z",
     "start_time": "2019-08-08T01:12:18.357215Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0808 14:11:51.631792 4765603264 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0808 14:11:51.722255 4765603264 deprecation.py:506] From <ipython-input-13-fe5860ceb47f>:33: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0808 14:11:51.845465 4765603264 deprecation.py:323] From /Users/luyaoye/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# we define our placeholders as the entrance of our data.\n",
    "# keep_prob is for the drop_out in the full_connect layer.\n",
    "user_id = tf.placeholder(tf.int32,[batch_size])\n",
    "item_id = tf.placeholder(tf.int32,[batch_size])\n",
    "rate = tf.placeholder(tf.float32,[batch_size,1])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "# We initialize our mlp user and item embedding matrix randomly here.\n",
    "# As our user id sart from 1 so the shape is [n_users + 1, emdedding_size], the same for items.\n",
    "user_embedding_mlp = tf.Variable(tf.random_uniform([n_users + 1, emdedding_size_mlp],0,0.3),trainable = True)\n",
    "item_embedding_mlp = tf.Variable(tf.random_uniform([n_items + 1, emdedding_size_mlp],0,0.3),trainable = True)\n",
    "\n",
    "#---------------------MLP Part-------------------#\n",
    "\n",
    "# We use embedding_lookup to find out the item and user we are training or testing now for the mlp layer,\n",
    "# both of them are with shape [bath_size, embedding_size_mlp].\n",
    "user_input_mlp = tf.nn.embedding_lookup(user_embedding_mlp, user_id)\n",
    "item_input_mlp = tf.nn.embedding_lookup(item_embedding_mlp, item_id)\n",
    "\n",
    "# Here is the regularization term using\n",
    "regularizer = tf.contrib.layers.l2_regularizer(0.2)\n",
    "reg_term = tf.contrib.layers.apply_regularization(regularizer,[user_input_mlp,item_input_mlp])\n",
    "\n",
    "# We multiply the user hidden layer with the item hidden layer to get their combination with shape [bath_size, embedding_size_mlp]\n",
    "fc_input = tf.concat([user_input_mlp,item_input_mlp],axis=1)\n",
    "\n",
    "# Then we put the combined vector into our fc layers, with an out put fc_output.\n",
    "# The shape of the fc_output is [batch_size, 1], which indicates the nonlinear connection between the item and the user.\n",
    "w_1 = tf.Variable(tf.truncated_normal([2 * emdedding_size_mlp, 5], stddev=0.1))\n",
    "b_1 = tf.Variable(tf.constant(0., shape=[5]))\n",
    "l_1 = tf.nn.tanh(tf.nn.xw_plus_b(fc_input, w_1, b_1))\n",
    "l_1_drop = tf.nn.dropout(l_1, keep_prob)\n",
    "\n",
    "\n",
    "w_2 = tf.Variable(tf.truncated_normal([5, 1], stddev=0.1))\n",
    "b_2 = tf.Variable(tf.constant(0., shape=[1]))\n",
    "prediction = tf.nn.xw_plus_b(l_1_drop,w_2,b_2)\n",
    "\n",
    "\n",
    "# We use mean_squared_error as the loss, and plus the regularization term.\n",
    "loss = tf.losses.mean_squared_error(rate, prediction) + reg_term\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:24:51.279067Z",
     "start_time": "2019-08-08T01:20:33.733305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning...\n",
      "epoch:  0\n",
      "Train_rmse: 3.047, Test_rmse: 1.200\n",
      "epoch:  1\n",
      "Train_rmse: 1.878, Test_rmse: 1.100\n",
      "epoch:  2\n",
      "Train_rmse: 1.561, Test_rmse: 1.026\n",
      "epoch:  3\n",
      "Train_rmse: 1.345, Test_rmse: 0.969\n",
      "epoch:  4\n",
      "Train_rmse: 1.178, Test_rmse: 0.920\n",
      "epoch:  5\n",
      "Train_rmse: 1.063, Test_rmse: 0.900\n",
      "epoch:  6\n",
      "Train_rmse: 0.992, Test_rmse: 0.895\n",
      "epoch:  7\n",
      "Train_rmse: 0.956, Test_rmse: 0.893\n",
      "epoch:  8\n",
      "Train_rmse: 0.936, Test_rmse: 0.894\n",
      "epoch:  9\n",
      "Train_rmse: 0.925, Test_rmse: 0.893\n",
      "epoch:  10\n",
      "Train_rmse: 0.918, Test_rmse: 0.892\n",
      "epoch:  11\n",
      "Train_rmse: 0.913, Test_rmse: 0.892\n",
      "epoch:  12\n",
      "Train_rmse: 0.910, Test_rmse: 0.892\n",
      "epoch:  13\n",
      "Train_rmse: 0.907, Test_rmse: 0.892\n",
      "epoch:  14\n",
      "Train_rmse: 0.905, Test_rmse: 0.895\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print('Start learning...')\n",
    "        for epoch in range(15):\n",
    "            print('epoch: ',format(epoch))\n",
    "            train_loss = []\n",
    "            for start,end in zip(range(0,len(train_data), batch_size),range(batch_size,len(train_data),batch_size)):\n",
    "                tr_loss,_ = sess.run([loss, train], feed_dict={user_id : train_data[start:end,0], item_id : train_data[start:end,1], rate: train_data[start:end,2].reshape(batch_size,1), keep_prob: 0.5})\n",
    "                train_loss.append(tr_loss)\n",
    "\n",
    "\n",
    "            rmse = []\n",
    "            for start, end in zip(range(0, len(test_data), batch_size),range(batch_size, len(test_data), batch_size)):\n",
    "                pred = sess.run(prediction, feed_dict={user_id : test_data[start:end,0], item_id : test_data[start:end,1], rate: test_data[start:end,2].reshape(batch_size,1), keep_prob: 1.0})\n",
    "                pred = [[min(max(i[0],1),5)] for i in pred]\n",
    "                pred = np.array(pred)\n",
    "                rmse.append((pred - test_data[start:end,2].reshape(batch_size,1))**2)\n",
    "\n",
    "            print(\"Train_rmse: {:.3f}, Test_rmse: {:.3f}\".format(np.sqrt(np.mean(train_loss)), np.sqrt(np.mean(rmse))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
