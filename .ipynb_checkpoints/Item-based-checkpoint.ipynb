{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-Based recommend system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:04:13.274343Z",
     "start_time": "2019-08-08T01:04:12.780031Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy as cp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Due to the algorithm of Item-based is logically same as the user-bases's. I just change the columns name of the rating data and put it into the algorithme of User-base.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:04:20.502873Z",
     "start_time": "2019-08-08T01:04:20.458082Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('ratings.csv')\n",
    "data = data[['userId', 'movieId', 'rating']]\n",
    "data.rename(columns={'userId':'movieId', 'movieId':'userId'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split\n",
    "For each movie in all movies, select fixed portion of users who watched this movie as train set and the rest as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:04:23.213802Z",
     "start_time": "2019-08-08T01:04:23.207629Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(data, seed = 6, portion = 0.8):\n",
    "    train_set = {}\n",
    "    test_set = {}\n",
    "    for user, movies in data.groupby('userId'):\n",
    "        movies = movies.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "        train = movies[:int(portion*len(movies)) + 1]\n",
    "        test = movies[int(portion*len(movies)) + 1:]\n",
    "        train_set[user] = train[['movieId', 'rating']]\n",
    "        test_set[user] = test[['movieId', 'rating']]\n",
    "    #print('Data preparation finished')\n",
    "    return train_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:04:24.987392Z",
     "start_time": "2019-08-08T01:04:24.981676Z"
    }
   },
   "outputs": [],
   "source": [
    "def itemUserAndRating(train_set):\n",
    "    item_user = {}\n",
    "    rating = {}\n",
    "    for u, items in train_set.items():\n",
    "        rating[u] = {}\n",
    "        movies = items['movieId'].tolist()\n",
    "        ratings = items['rating'].tolist()\n",
    "        for inum in range(len(movies)):\n",
    "            i = movies[inum]\n",
    "            rating[u][i] = ratings[inum]\n",
    "            if i not in item_user.keys():\n",
    "                item_user[i] = set()\n",
    "            item_user[i].add(u)\n",
    "    return item_user, rating\n",
    "\n",
    "\n",
    "# the number of actual seen movie\n",
    "#print(len(itemUserAndRating(train_set)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### theory\n",
    "1. Caculate the matrix of movie similarity based on cosin similarity\n",
    "2. For a specific combination of user and item, (u, i), in all the other movies which has been whatched by user u, choose the top K most similar movies to the movie i.\n",
    "3. Predict the score for such combination based on those top K similar movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:04:28.165854Z",
     "start_time": "2019-08-08T01:04:28.154480Z"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "def User_Similarity(train_set):\n",
    "    N = {}\n",
    "    C = {}\n",
    "    W = {}\n",
    "    for u, u_movie in train_set.items():\n",
    "        N[u] = u_movie.shape[0]\n",
    "        C[u] = {}\n",
    "        W[u] = {}\n",
    "        for v in train_set.keys():\n",
    "            C[u][v] = 0\n",
    "            W[u][v] = 0\n",
    "    # Build inverse table\n",
    "    item_user, rating = itemUserAndRating(train_set)\n",
    "    for i, users in item_user.items():\n",
    "        for u in users:\n",
    "            for v in users:\n",
    "                if v != u:\n",
    "                    urating = rating[u][i]\n",
    "                    vrating = rating[v][i]\n",
    "                    corating = -abs(urating - vrating) + 5\n",
    "                    C[u][v] += 1/np.log(1+len(users)*1.0) * corating\n",
    "    \n",
    "    # Calculate similarity matrix\n",
    "    for u, related_users in C.items():\n",
    "        for v, cuv in related_users.items():\n",
    "            W[u][v] = cuv/(N[u]*N[v])**0.5\n",
    "    print('user similarity finished')\n",
    "    return W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict all the possible ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:04:30.995102Z",
     "start_time": "2019-08-08T01:04:30.985782Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(train_set, K, user_sim):\n",
    "    # store the original ratings\n",
    "    item_user, orig_ratings = itemUserAndRating(train_set)\n",
    "\n",
    "    predict_ratings = cp.copy(orig_ratings)\n",
    "    for u, wu in user_sim.items():  # u is the user, wu is the user similarity list of u\n",
    "        # already_items is the movies already watched by u\n",
    "        already_items = train_set[u]['movieId'].tolist()\n",
    "        for item, vs in item_user.items():  # item is some movie, vs is the list of peopele who watched this movie\n",
    "            if item not in already_items:\n",
    "                # friendNum is the total amount of users we can use to predict for u with movie item\n",
    "                friendNum = min(K, len(vs))\n",
    "                wuv = [(v, wu[v]) for v in vs]\n",
    "                topv = sorted(wuv, key=itemgetter(1), reverse=True)[\n",
    "                    :friendNum]  # topv store the v id and wuv\n",
    "                sum_wuv = sum([_[1] for _ in topv])\n",
    "                if sum_wuv == 0:\n",
    "                    #nobody wathced this movie\n",
    "                    continue\n",
    "                topv_normal = [(_[0], _[1]/sum_wuv)\n",
    "                               for _ in topv]  # normalise the similarity\n",
    "                if item not in predict_ratings[u]:\n",
    "                    predict_ratings[u][item] = 0\n",
    "                for v in topv_normal:\n",
    "                    # the predict rating is the sum of product of related users's ratings and his similarity\n",
    "                    predict_ratings[u][item] += orig_ratings[v[0]][item] * v[1]\n",
    "        predict_ratings[u] = sorted(predict_ratings[u].items(), key = itemgetter(0))\n",
    "    print('prediction finished.')\n",
    "    return predict_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:04:33.665970Z",
     "start_time": "2019-08-08T01:04:33.662712Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmse(predict, real):\n",
    "    return np.sqrt(np.mean((predict-real)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:04:40.012734Z",
     "start_time": "2019-08-08T01:04:40.007839Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the performance score of the predictions\n",
    "def get_score(predictions, test_set):\n",
    "    rmselist = []\n",
    "    for user in test_set.keys():\n",
    "        test = test_set[user]\n",
    "        if test.shape[0] == 0:\n",
    "            continue\n",
    "        prediction = pd.DataFrame(predictions[user], columns=['movieId', user])\n",
    "        merge = pd.merge(prediction, test)\n",
    "        user_error = rmse(merge[user], merge['rating'])\n",
    "        #print(f\"For movie {user}, test error = {user_error}\")\n",
    "        rmselist.append(user_error)\n",
    "    return np.mean(rmselist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test the model and get the rmse score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:08:15.067408Z",
     "start_time": "2019-08-08T01:04:43.114185Z"
    }
   },
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(data)\n",
    "user_similarity = User_Similarity(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T01:27:31.152824Z",
     "start_time": "2019-08-08T01:10:42.717637Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = predict(train_set, 10, user_similarity)\n",
    "print(get_score(predictions, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do the cross validation and get a more convinceble error and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T15:07:25.474818Z",
     "start_time": "2019-08-07T15:07:25.465833Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(data, fold = 5, K = 20):\n",
    "    score_list = []\n",
    "    for seed in range(fold):\n",
    "        train_set, test_set = train_test_split(data, seed)\n",
    "        user_similarity = User_Similarity(train_set)\n",
    "        predictions = predict(train_set, K, user_similarity)\n",
    "        test_score = get_score(predictions, test_set)\n",
    "        print(f\"test:{seed}, rmse is {test_score}.\")\n",
    "        score_list.append(test_score)\n",
    "    return np.mean(score_list), np.var(score_list,ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T16:08:33.387111Z",
     "start_time": "2019-08-07T15:09:41.629479Z"
    }
   },
   "outputs": [],
   "source": [
    "print(cross_validation(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, we can see the bias of this recommender system is acceptable, and the variance is very small, which means this algorithm is stable.\n",
    "\n",
    "### Let's see how will the score change if we modify the K values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-08T02:24:49.550421Z",
     "start_time": "2019-08-08T01:29:24.550520Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_set, test_set = train_test_split(data)\n",
    "error = []\n",
    "x = []\n",
    "user_similarity = User_Similarity(train_set)\n",
    "for K in range(1, 16, 2):\n",
    "    predictions = predict(train_set, K, user_similarity)\n",
    "    test_score = get_score(predictions, test_set)\n",
    "    print(f\"K:{K}, rmse is {test_score}.\")\n",
    "    error.append(test_score)\n",
    "    x.append(K)\n",
    "plt.plot(x, error)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('rmse error')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "415px",
    "left": "1064px",
    "right": "20px",
    "top": "120px",
    "width": "356px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
